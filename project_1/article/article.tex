\documentclass[
    a4paper, aps, twocolumn, floatfix, superscriptaddress,
    nofootinbib]{revtex4-1}

% It's nice to be able to write your own name
\usepackage[T1]{fontenc}
% Automatic clickable links
\usepackage{hyperref}
% SI-units
\usepackage{siunitx}
% Enhanced math formatting
\usepackage{amsmath}
% Extended math symbols
\usepackage{amssymb}
% Include proof environment
\usepackage{amsthm}
% Import physics package to include bra-ket
\usepackage{physics}
\usepackage{enumerate}
% Import the tensor package for tensors
\usepackage{tensor}
% Include dod and dpd fracs
\usepackage{commath}
% Include font for the identity operator
\usepackage{dsfont}
% Include Tikz
\usepackage{tikz}
% Tikz add-on
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}
% Several figures in the same figure
\usepackage{subfig}
% Appendix
\usepackage[toc, page]{appendix}

% Macro for latin-letter vectors
\newcommand{\vf}{\mathbf}
% Macro for greek-letter vectors
\newcommand{\vfg}{\boldsymbol}

% Fast macro for real-numbers R
\newcommand{\R}{\mathbb{R}}
% Fast macro for complex-numbers C
\newcommand{\C}{\mathbb{C}}
% Fast macro for polynomial room P
\renewcommand{\P}{\mathbb{P}}
% New command for the identity operator
\newcommand{\1}{\mathds{1}}
% New command for the Lagrangian density
\newcommand{\cL}{\mathcal{L}}
% New command for the Hamiltonian density
\newcommand{\cH}{\mathcal{H}}
% Fast macro for partial differential tensors
\newcommand{\tpl}[1]{\tensor{\partial}{_#1}} % Lower
\newcommand{\tpu}[1]{\tensor{\partial}{^#1}} % Upper
% Fast macro for tensors
\newcommand{\te}[1]{\tensor{#1}}
% Fast macro for commutation and anti-commutation relations
\newcommand{\com}[2]{\left[#1, #2\right]}
\newcommand{\acom}[2]{\left\{#1, #2\right\}}

% Macros for writing auto-sized paranthesis, brackets and braces
\newcommand{\para}[1]{\left(#1\right)}
\newcommand{\brak}[1]{\left[#1\right]}
\newcommand{\brac}[1]{\left\{#1\right\}}

% Macro for creating orbital bra-ket's, i.e., bra-ket with paranthesis edges
\newcommand{\obra}[1]{( #1 \lvert}
\newcommand{\oket}[1]{\rvert #1 )}
\newcommand{\obraket}[2]{( #1 \lvert #2 )}

% Macro for expectation value
\newcommand{\expv}[1]{\langle #1 \rangle}

\newcommand{\half}{\frac{1}{2}}




\begin{document}

\title{Variational Monte Carlo on bosonic systems}
\author{Winther-Larsen, Sebastian Gregorius}
\homepage[Project code: ]{https://github.com/Schoyen/FYS4411}
\affiliation{University of Oslo}
\author{Schøyen, Øyvind Sigmundson}
\homepage[Project code: ]{https://github.com/Schoyen/FYS4411}
\affiliation{University of Oslo}
\date{\today}

\begin{abstract}
    In this project we use Quantum Variational Monte Carlo on bosonic systems.
    \begin{figure}[h!]
        \centering
        \includegraphics[width=0.7\textwidth]{loopholes.jpg}
    \end{figure}
\end{abstract}

\maketitle
\tableofcontents

%\begin{widetext}

%\end{widetext}

\section{Introduction}
    We will in this project study the Variational Monte Carlo method, and use it
    to evaluate the ground state energy of a trapped, hard sphere Bose gas.
    \cite{dubois2001bose} \cite{nilsen2005vortices}

    By building up an increasingly intricate system we eloquently illustrate the
    strength of the Variational Monte Carlo methods. Any Monte Carlo method
    draws on the fact that the computer will not complain when being tasked to
    perform a very similar task up to several million times in rapid succession.
    Monte Carlo methods is in essence a series of guesses where the current
    guess is only a minor alteration of the one prior to it. The trick lies in
    only accepting the subsequent guesses that improves upon our configuration
    up to a certain likelihood.

    The method we have employed is called a \emph{Variational} Monte Carlo
    method, because we make an educated guess at a trial wavefunction which we
    allow certain degrees of freedom called \emph{variational parameters}. We
    shall see that the system converges to a proper \footnote{Global, as far as
    we can tell.} minimum for only a certain set of parameters.

    First, we start by explaining our theoretical quantum system of a bosonic
    gas trapped in a potential well. Here we also outline the principle under
    which particles in the gas are allowed to interact. Moreover, we introduce
    the \emph{local energy} and the quantum \emph{drift force}. Here we also
    outline in more detailed algebra the nature of a non-interacting system. For
    this system we are able to derive an exact variational energy to which we
    can compare the results from our simulations. We furthermore give a
    description of the particle-particle interaction implemented as if each
    particle is a hard sphere. Second, we provide a thorough description of
    algorithms and statistical methods employed in this study. Here we introduce
    the Metropolis-Hastings algorithm. This algorithm can by a deft trick be
    expanded with the method of \emph{importance sampling} yielding a more
    intelligent choice for the next step in the Monte Carlo steps.

    The main point of note with regards to the statistical analysis is that the
    data generated by the Monte Carlo cycling is autocorrelated and computation
    of variance must be done with the \emph{blocking method}.  As an automated
    way of finding the optimal variational parameter, we sketch out the method
    of \emph{gradient descent}.  Third, we put the machinery described in the
    previous sections to work. We test against the actual values in a
    non-interacting system before analysing the more interesting perturbed-trap
    system with interacting particles.  Lastly, we highlight the more
    interesting aspects of this study in a discussion of the results before
    ending on some summary remarks.

\section{Theory}
    To model the trapped bosonic gas particles we use the potential
    \begin{align}
        v(\vf{r})
        &=
        \begin{cases}
            \half m\omega^2r^2 & (\text{S}), \\
            \half m \bigl[
                \omega^2(x^2 + y^2) + \omega_z^2z^2
            \bigr] & (\text{E}),
        \end{cases}
    \end{align}
    where we can choose between a spherical (S) or an elliptical (E) harmonic
    oscillator trap.  In the latter potential the oscillatory frequency is
    different in the $z$-direction, i.e., $\omega_z \neq \omega$.  The full
    Hamiltonian of the system is given by
    \begin{align}
        H = \sum_{i = 1}^{N}h(\vf{r}_i) + \sum_{i < j}^{N}w(\vf{r}_i, \vf{r}_j),
    \end{align}
    where the single particle one-body operator, $h(\vf{r}_i)$, is given by
    \begin{align}
        h(\vf{r}_i) = -\frac{\hbar^2}{2m}\nabla_i^2
        + v(\vf{r}_i),
    \end{align}
    for equal masses, and the two-body interaction operator, $w(\vf{r}_i,
    \vf{r}_j)$, is
    \begin{align}
        w(\vf{r}_i, \vf{r}_j)
        = \begin{cases}
            \infty & |\vf{r}_i - \vf{r}_j| \leq a, \\
            0 & |\vf{r}_i - \vf{r}_j| > a,
        \end{cases}
        \label{eq:two-body_interaction}
    \end{align}
    where $a$ is the hard sphere of the particle.  The trial wavefunction,
    $\Psi_T(\vf{r})$, we will be looking at is given by
    \begin{align}
        \Psi_T(\vf{r})
        &= \Phi_T(\vf{r})
        \prod_{j < k}^N f(a, \vf{r}_j, \vf{r}_k) \\
        &= \Biggl(
            \prod_{i = 1}^N g(\alpha, \beta, \vf{r}_i)
        \Biggr)
        \prod_{j < k}^N f(a, \vf{r}_j, \vf{r}_k),
        \label{eq:initial_trial_wavefunction}
    \end{align}
    where $\alpha$ is a variational parameter and
    \begin{align}
        \vf{r} = (\vf{r}_1, \vf{r}_2, \dots, \vf{r}_N, \alpha, \beta).
    \end{align}
    Here $g$ are the single particle wavefunctions given by
    \begin{align}
        g(\alpha, \beta, \vf{r}_i)
        = \exp\bigl[
            -\alpha(x_i^2 + y_i^2 + \beta z_i^2)
        \bigr] \equiv \phi(\vf{r}_i),
    \end{align}
    and $\Phi_T(\vf{r})$ is the \emph{Slater permanent} consisting of the $N$
    first single particle wavefunctions, and the correlation wavefunction, $f$,
    given by
    \begin{align}
        f(a, \vf{r}_j, \vf{r}_k)
        &=
        \begin{cases}
            0 & |\vf{r}_j - \vf{r}_k| \leq a, \\
            \Bigl(
                1 - \frac{a}{|\vf{r}_j - \vf{r}_k|}
            \Bigr) & |\vf{r}_j - \vf{r}_k| > a.
        \end{cases}
        \label{eq:correlation_wavefunction}
    \end{align}
    We will for brevity use the notation $\phi(\vf{r}_i) = \phi_i$ and $r_{jk} =
    |\vf{r}_j - \vf{r}_k|$. In practice \autoref{eq:two-body_interaction} is
    unnecessary as the situtation where $r_{jk} \leq a$ automatically yields a
    probability density of zero from \autoref{eq:correlation_wavefunction} thus
    rejecting the state completely.

    \subsection{Local energy}
        As the many-body wavefunction creates a very large configuration space,
        where much of the wavefunction is small, we use the Metropolis algorithm
        in order to move towards regions in configuration space with
        ``sensible'' values. We define the \emph{local energy}, $E_L(\vf{r})$,
        by
        \begin{align}
            E_L(\vf{r})
            &= \frac{H\Psi_T(\vf{r})}{\Psi_T(\vf{r})}.
        \end{align}
        If $\Psi_T(\vf{r})$ is an exact eigenfunction of the Hamiltonian, $E_L$
        will be constant. The closer $\Psi_T(\vf{r})$ is to the exact wave
        function, the less variation in $E_L$ as a function of $\vf{r}$ we get.
        This leads us to searching for values of $\vf{r}$ yielding the lowest
        variance in $E_L$.

        When we are performing the Monte Carlo sampling we are interested in the
        expected value of the local energy as this will serve as our estimate of
        the true energy. Mathematically this is expressed as
        \begin{align}
            \expval{E_L}
            &=
            \frac{\int\dd \vf{r}\Psi_T^{*}(\vf{r})H\Psi_T{\vf{r}}}
            {\int\dd \vf{r}|\Psi_T(\vf{r})|^2}
            =
            \frac{
                \int\dd \vf{r}|\Psi_T(\vf{r})|^2 E_L
            }{
                \int\dd \vf{r}|\Psi_T(\vf{r})|^2
            },
        \end{align}
        where we have multiplied and divided by the trial wavefunction to get
        the latter expression.

        One of the most computationally intensive parts of the Variational Monte
        Carlo algorithm will be to compute $E_L$. We will therefore find an
        analytical expression for $E_L$ in terms of the trial wavefunctions and
        the system we are exploring.

    \subsection{The drift force}
        A disadvantage in the use of the brute force Metropolis-Hastings
        algorithm is that we might be spending much computational resources in
        an uninteresting part of configuration space. To make smarter moves we
        will use the Metropolis-Hastings algorithm with the additions of
        \emph{Importance Sampling}  (which will be discussed in due time). This
        alteration of the algorithm is dependent on the \emph{drift force} of
        the system.
        \begin{align}
            \vf{F}(\vf{r})
            &=
            \sum_{k = 1}^N
            \vf{F}_k(\vf{r})
            =
            \sum_{k = 1}^N
            \frac{2\nabla_k\Psi_T(\vf{r})}{\Psi_T(\vf{r})}.
            \label{eq:drift_force}
        \end{align}
        Using this expression we are able to move towards parts of
        configuration space where the gradient increases or decreases yielding a
        better choice of movements. We will mainly be interested in the drift
        force of a single particle $k$.

\subsection{Non-interacting harmonic oscillators}
    We start by looking at a simple system of non-interacting harmonic
    oscillators. That is, where $a = 0$ and $\beta = 1$, i.e. there is no hard-shell
    interaction between particles and the particles are confined in a spherical trap.
    We thus get the trial  wavefunction
    \begin{align}
        \Psi_T(\vf{r})
        = \Phi_T(\vf{r})
        = \prod_{i = 1}^N \exp\bigl[
            -\alpha |\vf{r}_i|^2
        \bigr],
    \end{align}
    where $|\vf{r}_i| = r_i$. As $a = 0$ the interaction term,
    $w(\vf{r}_i, \vf{r}_j)$, vanishes and the Hamiltonian is given by
    (in the spherical case)
    \begin{align}
        H &= \sum_{i = 1}^N h(\vf{r}_i)
        = \sum_{i = 1}^N \Biggl(
            -\frac{\hbar^2}{2m}\nabla_i^2
            + \half m \omega^2 r_i^2
        \Biggr).
    \end{align}
    To find the drift force and the local energy we have to compute the gradient
    and the Laplacian of the trial wavefunction. The gradient is given by
    \begin{align}
        \nabla_k\Psi_T(\vf{r})
        &= -2\alpha \vf{r}_k\Psi_T(\vf{r}),
    \end{align}
    whereas the Laplacian yields
    \begin{align}
        \nabla^2_k\Psi_T(\vf{r})
        &= \big(-2d\alpha + 4\alpha^2 r_k^2\bigr)\Psi_T(\vf{r}),
    \end{align}
    where $d$ is the dimensionality of the problem determined by
    $\vf{r}_k \in \mathbb{R}^d$. We can thus use the gradient to find an
    expression for the drift force for particle $k$.
    \begin{align}
        \vf{F}_k(\vf{r})
        &= -4\alpha\vf{r}_k.
    \end{align}
    Using the Laplacian we can compute the kinetic term in the expression for the
    local energy. We get
    \begin{align}
        E_L(\vf{r})
        &=
        \sum_{i = 1}^N
        \Biggl(
            -\frac{\hbar^2}{2m}
            \bigl[
                -2d\alpha + 4\alpha^2 r_i^2
            \bigr]
            + \half m\omega^2 r_i^2
        \Biggr).
    \end{align}
    In natural units, with $\hbar = c = m = 1$, this reduces to
    \begin{align}
        E_L(\vf{r})
        &=
        \alpha dN
        + \biggl(
            \half\omega^2
            - 2\alpha^2
        \biggr)
        \sum_{i = 1}^N r_i^2.
        \label{eq:closed_form_natural_units_local_energy}
    \end{align}
    It is worth noting that for $\alpha = \half\omega$ ($\alpha$ is required to
    be positive) we will find a stable value which turns out to be the exact
    energy minimum. This happens as the sum over all particles
    disappears.

    \subsubsection{Exact variational energy}
        As the system is non-interacting and consisting of Gaussians we can
        find an expression for the exact energy as a function of the
        variational parameter $\alpha$, i.e.,
        \begin{align}
            E(\alpha)
            &=
            \frac{\bra{\Psi_T}H\ket{\Psi_T}}{\bra{\Psi_T}\ket{\Psi_T}}.
        \end{align}

        % TODO: Add the computation of this expression
        The final result for the energy is
        \begin{align}
            E(\alpha)
            &=
            \left(
                \frac{\hbar^2 \alpha}{2m}
                + \frac{m\omega^2}{8\alpha}
            \right)dN.
            \label{eq:exact_energy}
        \end{align}
        By minimizing this expression, i.e., finding the derivative of the
        energy with respect to $\alpha$ and equating this to zero, yields the
        expected minimum of variational energy to be
        \begin{align}
            \dod[]{E(\alpha)}{\alpha} = 0
            \implies
            \alpha_0 = \frac{m\omega}{2\hbar},
        \end{align}
        which in natural units reduces to $\alpha_0 = \half\omega$. The energy at
        this value of $\alpha$ (in natural units) is then
        \begin{align}
            E(\alpha_0)
            &=
            \frac{\omega dN}{2}.
        \end{align}


\subsection{Interacting hard sphere bosons}
    Moving to the full system allowing $\beta$ to vary and setting $a
    \neq 0$ we can write the trial wavefunction as
    \begin{align}
        \Psi_T(\vf{r})
        &=
        \Phi_T(\vf{r})
        J(\vf{r}),
    \end{align}
    where $\ket{\Phi_T}$ is the same Slater permanent as in
    \autoref{eq:initial_trial_wavefunction} and $J(\vf{r})$ is the
    \emph{Jastrow factor} given by
    \begin{align}
        J(\vf{r})
        &=
        \exp\Biggl(
            \sum_{j < l}^N u(r_{jl})
        \Biggr),
    \end{align}
    where $r_{jk} = |\vf{r}_j - \vf{r}_k|$ and
    \begin{align}
        u(r_{jk}) = \ln\bigl[f(a, \vf{r}_j, \vf{r}_k)\bigr].
    \end{align}
    To further shorten the notation we will use $u_{jk} = u(r_{jk})$. Computing
    the gradient of the wavefunction we get
    \begin{align}
        \nabla_k\Psi_T(\vf{r})
        &=
        \Bigl[
            \nabla_k
            \Phi_T(\vf{r})
        \Bigr]
        J(\vf{r})
        + \Phi_T(\vf{r})
        \nabla_k J(\vf{r}).
    \end{align}
    The gradient of the Slater permament for particle $k$ is given by
    \begin{align}
        \nabla_k
        \Phi_T(\vf{r})
        &=
        \nabla_k\phi_k
        \prod_{i \neq k}^N\phi_i
        = \frac{\nabla_k\phi_k}{\phi_k}
        \Phi_T(\vf{r}).
    \end{align}
    The gradient of the Jastrow factor is given by
    \begin{align}
        \nabla_k J(\vf{r})
        &=
        J(\vf{r})
        \nabla_k\sum_{m < n}^N u_{mn} \\
        &= J(\vf{r})
        \Biggl(
            \sum_{m = 1}^{k - 1}\nabla_k u_{mk}
            \sum_{n = k + 1}^N\nabla_k u_{kn}
        \Biggr)
        \\
        &=
        J(\vf{r})
        \sum_{m \neq k}^N\nabla_k u_{km},
    \end{align}
    where the gradient of the interaction term splits the antisymmetric
    sum into two parts. As $r_{ij} = r_{ji}$ we can combine these sums
    into a single sum. This in total yields the gradient
    \begin{align}
        \nabla_k\Psi_T(\vf{r})
        &=
        \Biggl(
            \frac{\nabla_k\phi_k}{\phi_k}
            + \sum_{m \neq k}^N
            \nabla_k u_{km}
        \Biggr)
        \Psi_T(\vf{r}).
        \label{eq:gradient_full_wavefunction}
    \end{align}

    The Laplcian of the trial wavefunction is found by finding the divergence of
    \autoref{eq:gradient_full_wavefunction}.
    \begin{align}
        \nabla_k^2\Psi_T(\vf{r})
        &=
        \Biggl(
            \nabla_k\Biggl[
                \frac{\nabla_k\phi_k}{\phi_k}
            \Biggr]
            +
            \sum_{m \neq k}^N \nabla_k^2 u_{km}
        \Biggr)\Psi_T(\vf{r})
        \\
        &\qquad
        +
        \Biggl(
            \frac{\nabla_k\phi_k}{\phi_k}
            + \sum_{m \neq k}^N
            \nabla_k u_{km}
        \Biggr)^2
        \Psi_T(\vf{r}),
    \end{align}
    where the squared term came from taking the gradient of the trial
    wavefunction.  To further simplify we divide by the trial
    wavefunction. This yields
    \begin{align}
        \frac{\nabla_k^2\Psi_T(\vf{r})}{\Psi_T(\vf{r})}
        &=
        \frac{\nabla_k^2\phi_k}{\phi_k}
        + 2\frac{\nabla_k\phi_k}{\phi_k}
        \sum_{m \neq k}\nabla_k u_{km}
        \nonumber \\
        &\qquad
        + \sum_{m\neq k}^N\nabla_k^2 u_{km}
        + \Biggl(
            \sum_{m \neq k}^N\nabla_k u_{km}
        \Biggr)^2.
    \end{align}
    From here, the next step is to find the gradient and the Laplacian of
    the single particle functions, $\phi_k$, and the interaction
    functions $u_{km}$. For the single particle functions we use
    Cartesian coordinates when finding the derivatives while for
    the interaction functions we use spherical coordinates and do a
    variable substitution. Beginning with the gradient of the single
    particle functions we get
    \begin{align}
        \nabla_k\phi_k
        &=
        \nabla_k\exp\bigl[
            -\alpha(x_k^2 + y_k^2 + \beta z_k^2)
        \bigr] \\
        &=
        -2\alpha
        (x_k\vf{e}_i + y_k\vf{e}_j + \beta z_k\vf{e}_k)
        \phi_k.
    \end{align}
    Note that the subscripts on the unit vectors $\vf{e}_i$ are
    \emph{not} the same as the subscripts used for its components. The
    Laplacian yields
    \begin{align}
        \nabla_k^2\phi_k
        &=
        \Bigl[
            -2\alpha
            \big(
                d - 1 + \beta
            \bigr)
            \nonumber \\
            &\qquad
            + 4\alpha^2
            \bigl(
                x_k^2 + y_k^2 + \beta^2z_k^2
            \bigr)
        \Bigr]
        \phi_k,
    \end{align}
    with $d$ as the dimensionality of the problem.
    In order to derive the interaction functions we have to do a
    variable substitution using $r_{km} = |\vf{r}_k - \vf{r}_m|$. We can
    then rewrite the $\nabla_k$-operator as
    \begin{align}
        \nabla_k
        &=
        \nabla_k
        \dpd[]{r_{km}}{r_{km}}
        =
        \nabla_k r_{km} \dpd[]{}{r_{km}}
        \\
        &=
        \frac{\vf{r}_k - \vf{r}_m}{r_{km}}\dpd[]{}{r_{km}}.
    \end{align}
    Applying this version of the $\nabla_k$-operator to $u_{km}$ yields
    \begin{align}
        \nabla_k u_{km}
        &=
        \frac{\vf{r}_k - \vf{r}_m}{r_{km}}
        \dpd[]{u_{km}}{r_{km}}.
    \end{align}

    \begin{widetext}
        For the Laplacian we switch a little back and forth between the
        two ways of representing the $\nabla_k$-operator. We thus get
        \begin{align}
            \nabla_k^2 u_{km}
            &=
            \frac{\nabla_k \vf{r}_k}{r_{km}}\dpd[]{u_{km}}{r_{km}}
            + \Biggl[
                \nabla_k \frac{1}{r_{km}}
            \Biggr]
            (\vf{r}_k - \vf{r}_m)\dpd[]{u_{km}}{r_{km}}
            + \frac{\vf{r}_k - \vf{r}_m}{r_{km}}
            \nabla_k \dpd[]{u_{km}}{r_{km}} \\
            &= \frac{d}{r_{km}}\dpd[]{u_{km}}{r_{km}}
            - \frac{(\vf{r}_k - \vf{r}_m)^2}{r_{km}^3}
            \dpd[]{u_{km}}{r_{km}}
            + \frac{(\vf{r}_k - \vf{r}_m)^2}{r_{km}^2}
            \dpd[2]{u_{km}}{r_{km}} \\
            &=
            \frac{d - 1}{r_{km}}\dpd[]{u_{km}}{r_{km}}
            + \dpd[2]{u_{km}}{r_{km}},
        \end{align}
        where $d$ is again the dimensionality of the problem. In total
        we can state an intermediate version of the Laplacian occuring
        in the local energy as
        \begin{align}
            \frac{\nabla_k^2\Psi_T(\vf{r})}{\Psi_T(\vf{r})}
            &=
            \frac{\nabla_k^2\phi_k}{\phi_k}
            + 2\frac{\nabla_k\phi_k}{\phi_k}
            \sum_{m \neq k}^N
            \frac{\vf{r}_k - \vf{r}_m}{r_{km}}
            \dpd[]{u_{km}}{r_{km}}
            + \sum_{m\neq k}^N
            \Biggl(
                \frac{d - 1}{r_{km}}\dpd[]{u_{km}}{r_{km}}
                + \dpd[2]{u_{km}}{r_{km}}
            \Biggr)
            \nonumber \\
            &\qquad
            +
            \sum_{m, n \neq k}^N
            \frac{\vf{r}_k - \vf{r}_m}{r_{km}}
            \frac{\vf{r}_k - \vf{r}_n}{r_{kn}}
            \dpd[]{u_{km}}{r_{km}}
            \dpd[]{u_{kn}}{r_{kn}}.
        \end{align}
    \end{widetext}
    Moving on to the derivatives of the interaction terms, $u_{km}$, to
    get an explicit expression for the Laplacian.
    \begin{align}
        \dpd[]{u_{km}}{r_{km}}
        &=
        \frac{a}{r_{km}(r_{km} - a)},
        \\
        \dpd[2]{u_{km}}{r_{km}}
        &= \frac{a^2 - 2ar_{km}}{r_{km}^2(r_{km} - a)^2}.
    \end{align}
    The local energy and the drift force can now be found by combining these
    expressions. We will not write out the explicit expressions as
    these will be called by separated functions in our programs.

    \subsubsection{Scaling the system}
        We now introduce a scaled distance $\vf{r}' = \vf{r}/a_{\text{ho}}$
        \cite{dubois2001bose}, where
        \begin{align}
            a_{\text{ho}}
            &=
            \sqrt{\frac{\hbar}{m\omega}}.
        \end{align}
        We can then rewrite the Hamiltonian for the elliptic potential in terms
        of this new distance. By doing a variable substitution for each
        direction in the Laplace operator we get
        \begin{align}
            \nabla_k^2
            &=
            \frac{1}{a_{\text{ho}}^2}{\nabla_k'}^2.
        \end{align}

        Looking at the one-body part of the Hamiltonian we get
        \begin{align}
            H
            &=
            \sum_{k = 1}^N
            \brac{
                -\frac{\hbar^2}{2m}\nabla_k^2
                + \half m\brak{
                    \omega^2(x_k^2 + y_k^2)
                    + \omega_z^2 z_k^2
                }
            }
            \\
            &=
            \frac{\hbar\omega}{2}
            \sum_{k = 1}^N
            \brac{
                -{\nabla_k'}^2
                +
                \brak{
                    {x_k'}^2 + {y_k'}^2 + \lambda^2{z_k'}^2
                }
            },
            \label{eq:scaled_hamiltonian}
        \end{align}
        where we have introduced the dimensionless frequency $\lambda =
        \omega_z/\omega$. The single particle functions also gets a scaling
        factor
        \begin{align}
            \phi_k
            &=
            \exp\brak{
                -\frac{\alpha\hbar}{m\omega}
                ({x_k'}^2 + {y_k'}^2 + \beta^2{z_k'}^2)
            }
            \equiv
            \phi(\vf{r}_k').
            \label{eq:scaled_spf}
        \end{align}
        The correlation wavefunction and the interaction potential remain the
        same, but using the scaled hard core sphere radius $a/a_{\text{ho}} =
        0.0043$. For brevity we will remove the ticks on the variables.


\section{Algorithms}
    In this project we rely on a Monte Carlo approach of random sampling to
    obtain numerical results.  We simulate random walks over a volume in order
    to find optimal parameters in our trial wavefunctions.  The most common of
    such methods, which we make use of herein, is the Metropolis-Hastings
    algorithm.

    \subsection{The Metropolis-Hastings Algorithm}
        The Metropolis-Hastings algorithm can in our particular situation be
        condensed down to the following steps:

        \begin{enumerate}
            \item The system is initialised by a certain number $N$ of randomly
                generated positions, or particles. This allows us to evaluate
                the wavefunction at these points and compute the local energy
                $E_L$.

            \item The initial configuration is changed by setting a new position
                for one of these particles. The particle is picked at random.

            \item A ratio between new wavefunction density and the previous
                (initial) density is computed and compared to a random number.
                This acceptance probability decides if the particle move is
                rejected or accepted.  The particle is only allowed to move a
                predetermined step length.

            \item If the particle movement is accepted and the local energy
                $E_L$ is computed for the new system.

            \item Repeat steps until convergence and an optimum is reached.

        \end{enumerate}

        The algorithm described above can be applied in an "exhaustive" search
        of the parameter space in order to find the optimal parameters.  Whether
        a proposed move is accepted or not is determined by a transition
        probability and the acceptance probability.  The strength of the
        algorithm is that the transition algorithm need not be known. For
        example, the simplest case is to accept the new state, i.e., the new
        position for the random walker, if the ratio
        \begin{align}
            q(\vf{r}_{i + 1}, \vf{r}_i)
            &=
            \frac{\left|\Psi_T(\vf{r}_{i + 1})\right|^2}
            {\left|\Psi_T(\vf{r}_{i})\right|^2},
        \end{align}
        where $\vf{r}_{i + 1}$ are all the positions at step $i + 1$, is greater
        than a uniform probability $p \in [0, 1)$.

        \subsubsection{Importance Sampling}
            A problem with the naïve Metropolis-Hastings sampling approach is
            that the sampling of position space is done with no regard for where
            we are likely to find a particle. This problem can be remedied
            through \emph{importance sampling}.  It is reasonable to assume that
            the particles we erratically scatter in space are prone to movement
            towards the peaks of the probability density as dictated by the wave
            function. Consider therefore the Fokker-Planck equation,
            \begin{align}
                \dpd[]{\Psi_T}{t}
                &=
                D \nabla\cdot
                \left(
                    \nabla
                    - \vf{F}
                \right) \Psi_T,
                \label{eq:fokker_planck}
            \end{align}
            which describes the evolution in time of a probability density
            function. In our case this is the trial wavefunction $\Psi_T$.
            Originaly an equation that models diffusion, we have a diffusion
            term $D$ and the drift force \autoref{eq:drift_force}. In our case
            the diffusion term $D$ is simply $1/2$ from the kinetic energy (in
            natural units).

            We use the Langevin equation to find the new position of the
            particle.
            \begin{align}
                \dpd[]{\vf{r}}{t}
                &=
                D\vf{F}(\vf{r}) + \vfg{\eta},
                \label{eq:langevin}
            \end{align}
            where $\vfg{\eta}$ is a uniformly distributed stochastic variable
            for each dimenion.  Solving Langevin's equation by Euler's method
            gives a recursive relation for the subsequent new positions of a
            particle.
            \begin{align}
                \vf{r}_{i + 1}
                &=
                \vf{r}_i + D\vf{F}(\vf{r}_i)\delta t
                + \vfg{\xi}\sqrt{\delta t},
            \end{align}
            for a given time step $\delta t$
            \footnote{
                Bear in mind that \autoref{eq:langevin} is only valid as $\Delta
                t \to 0$, a property stemming from the use of Euler's method.
            }
            and a normally distributed stochastic variable $\vfg{\xi}$.

            Now we need to change the acceptance probability of the metropolis
            algorithm to something that takes the new sampling method into
            account.
            \begin{align}
                q(\vf{r}_{i + 1}, \vf{r}_i)
                &=
                \frac{
                    G(\vf{r}_{i + 1}, \vf{r}_{i}, \delta t)
                    \left|\Psi_T(\vf{r}_{i + 1})\right|^2
                }
                {
                    G(\vf{r}_{i}, \vf{r}_{i + 1}, \delta t)
                    \left|\Psi_T(\vf{r}_{i})\right|^2
                },
            \end{align}
            where $G$ is the Green's function of the Fokker-Planck equation
            given by
            \begin{align}
                G(\vf{r}_{i + 1}, \vf{r}_i, \delta t)
                &=
                \exp\Biggl(
                    -\frac{\bigl[
                        \vf{r}_{i + 1}
                        - \vf{r}_{i}
                        - D\vf{F}(\vf{r}_i)\delta t
                    \bigr]^2}{4D\delta t}
                \Biggr)
                \nonumber \\
                &\qquad
                \times
                \frac{1}{(4\pi D\delta t)^{dN/2}},
            \end{align}
            where $d$ is the dimensionality.

    \subsection{Statistical Analysis}
        If the results of the metropolis sampling were completely uncorrelated,
        it would be enough to compute the standard deviation in a familiar way,
        \begin{equation}
            \sigma = \sqrt{\frac{1}{N}(\ev{E^2_L} - \ev{E_L}^2},
            \label{eq:standard_deviation}
        \end{equation}
        where $N$ is the number of samples, or Monte-Carlo cycles, in the experiment.
        However, it is reasonable to assume that the data we are dealing with in this
        study is liable to suffer from \emph{autocorrelation} and \autoref{eq:standard_deviation}
        does not hold. The prevailing
        definition of autocorrelation in a data stream or signal is correlation
        between a delay of the signal and the original signal. One would be interested to find the
        delay, or lag, in the signal at which the "self-correlation" is highest. We refer to this
        spacing as $d$, and define the following correlation function,
        \begin{equation}
            f_d = \frac{1}{n -d} \sum_{k=1}^{n-d} (x_k - \bar{x}_n)(x_{k+d} - \bar{x}_n).
            \label{eq:correlation_f_d}
        \end{equation}
        The keen reader would have noticed that the function $f_d$ in \autoref{eq:correlation_f_d}
        would be equal to the sample variance for $d=0$. We can now define the \emph{autocorrelation function}
        \begin{equation}
            \kappa_d = \frac{f_d}{\text{Var}(x)},
            \label{eq:autocorrelation_function}
        \end{equation}
        which is equal to $1$ if the data exhibits no autocorrealtions, equating to $d=0$. From the autocorrelation
        function (\ref{eq:autocorrelation_function}) we in turn define the \emph{autocorrelation time},
        \begin{equation}
            \tau = 1 + 2 \sum_{d=1}^{n-1}\kappa_d,
        \end{equation}
        notice that the autocorrelation time is $1$ for a correlation free experiment.

        We are now able to make a correction to the expression for the standard deviation
        improving on \autoref{eq:standard_deviation} by taking correlation into account,
        \begin{equation}
            \sigma =\sqrt{\frac{1 + 2\tau / \Delta t}{N}(\ev{E^2_L} - \ev{E_L}^2)},
        \end{equation}
        where $\Delta t$ is the time between each sample. The main problem at this point is to
        find $\tau$, which we do not know for any given system and it is generally very expensive to
        compute. In order to find a good estimate of $\tau$ we use a procedure called \emph{blocking}.

        \subsubsection{Blocking}
        In the method of blocking we group the samples into blocks of increasing size. If one where to compute
        the standard deviation for each block, one should see the variance increase with the block size.
        The standard deviation would only increase up to a certain point, from whence it would stay
        almost constant. What is happening is that we have reached a point where a particular
        sample from one block is no longer correlated with a corresponding sample from an adjacent block.
        The block size for this point of convergence now functions as an estimate for the autocorrelation
        time $\tau$.

         Instead of going by this "manual" method of looking at charts, we will construct a test statistic and
         let the computer do the necessary considerations in a more "automatic way.

        The easiest way to do this is by use of the \emph{automatic blocking scheme}, nicely illustrated
        with a flow chart in \autoref{fig:blocking}. The input parameter for the method is an array of the
        local energies, but it can be any array of time series data, which is why it is herein referred to as
        $\mathbf{X}$. Any array of sequential data, where any particular data point depends on the previous
        data point inhibits all attributes of a time series. Our array of local energies is therefor most definitely
        a time series.

        From $\mathbf{X}$ we compute the variance and sample covariance between the series itself and a
        one-point lag of the series. This covariance is usually referred to as the first order autocovariance.
        \begin{align}
            \gamma_i
            &= \frac{1}{n}
            \left(
                \sum_{k=1}^n
                \left((\mathbf{X}_i)_k - (\bar{\mathbf{X}}_i)_k\right)
            \right)
            \nonumber \\
            &\qquad
            \times
            \left(
                \sum_{k=0}^{n-1}
                \left((\mathbf{X}_i)_k - (\bar{\mathbf{X}}_i)_k\right)
            \right).
        \end{align}

        Now we "block"! The next step is to perform a transformation of the our time series $\mathbf{x}_i$, where
        we end up with an array that is half the length of the original time series,
        \begin{equation}
            (\mathbf{X}_i)_k = \frac{1}{2}\left((\mathbf{X}_i)_{2k-1} + (\mathbf{X}_i)_{2k} \right).
        \end{equation}
        This process is were the term "blocking" stems from. There are a number of ways to perform the blocking -
        we could have picked the data points used in the new ($i+1$) array randomly, or more orderly. Here we we have
        picked them sequentially. We have obtained $\frac{n}{2}$ new "primed" stochastic variables. Assuming that the
        reader is playing close attention, it is now needless to say that it is much easier to perform these sequential
        blocking steps if we have an array of length $n= 2^d$, where $d$ is integer. Otherwise, problems would arise
        because one would have arrays of different lengths when the original array is "split in two". The ailment is
        easy to remedy by excluding an observation from the longest resulting array after splitting.

        For each new time series $\mathbf{X}_{i+1}$ a new sample variance and sample covariance is computed.
        The blocking transformation is allowed to continue until we are we run out of data, i.e. the length
        of $\mathbf{X}_i < 2$. In other words, there is only a single number left. We now compute a test statistic,
        \begin{equation}
            M_j = \sum_{k=j}^{d-1} n_k \frac{\left[(n_k - 1)\frac{s^2_k}{n_k^2} y \gamma_k(1)\right] ^2}{s_k^4}.
        \end{equation}
        As luck would have it, this number $M$ is   $\chi^2$-distributed. The following step would be to look up a
        statistical table and find the first $k$, such that $M_k \leq q_{d-k}(1-\alpha)$, where $q$ is the statistical
        limit and $\alpha$ is  the significance level that your hear desires. Finally, the correct sample standard deviation
        is $\frac{s_k^2}{n_k}$. Again, see \autoref{fig:blocking} for a summary of the algorithm.

        % Flow chart for description of automatic blocking algorithm
        \begin{figure}
            \centering
            % Defining blocks of flowchart
            \tikzstyle{decision} = [diamond, draw, fill=blue!20,
                text width=5em, text badly centered, node distance=3cm, inner sep=0pt]
            \tikzstyle{block} = [rectangle, draw, fill=blue!20,
                    text width=11em, text centered, rounded corners, minimum height=4em]
            \tikzstyle{line} = [draw, -latex']

            % Actual figure
            \begin{tikzpicture}[node distance = 2.2cm, auto]

                % Nodes
                \node [block] (init) {Start with $\mathbf{X}_i$, $i=0$};
                \node [block, below of=init] (variance) {Compute variance and autocovariance of $\mathbf{X}_i$, $s_i$ and $\gamma_i$};
                \node [block, below of=variance] (transform)
                    {Transform data $(\mathbf{X}_{i+1})_k = \frac{1}{2}\left((\mathbf{X})_i)_{2k-1} + (\mathbf{X}_i)_{2k} \right)$};
                \node [block, right=1.0cm of transform] (i+1) {Next iteration: $i = i + 1$.} ;
                \node [decision, below of = transform] (x_size) {Length of $\mathbf{X}_{i + 1} \geq 2$ ? };
                \node [block, below of=x_size] (m_test) {Compute test statistic $M_j$ from $s_j$ and $\gamma_j$ for all $j$.};
                \node [block, below of=m_test] (magic_numbers) {Find first $k$ so that $M_k \leq q_{d-k}(1-\alpha)$};
                \node [block, below of=magic_numbers] (return) {Return $s_k^2/n_k$};

                % Edges
                \path [line] (init) -- (variance);
                \path [line] (variance) -- (transform);
                \path [line] (transform) -- (x_size);
                    \path [line] (x_size) -| node [near start] {yes} (i+1);
                    \path [line] (i+1) |- (variance);
                \path [line] (x_size) -- node [near start] {no} (m_test);
                \path [line] (m_test) -- (magic_numbers);
                \path [line] (magic_numbers) -- (return);
            \end{tikzpicture}
            \caption{Flow chart depicting the automatic blocking method for finding a better estimate for sample variance.}
            \label{fig:blocking}
        \end{figure}

        \subsection{Gradient Descent}

            Scanning the entire space of variational parameters can be a
            somewhat despairing task. To aid us in the search we therefore
            implement the method of gradient descent\footnote{Also known as
            steepest descent, easily confused with the steepest descent method
            for integation.}. In any optimum we generally have that the
            derivative should equal zero. In our case, we want to find the
            minimum of the expected local energy with respect to some
            variational parameter $\alpha$, so that
            \begin{equation}
                \nabla_\alpha \expval{E_L(\mathbf{R}; \alpha)} = 0.
            \end{equation}
            The foundation for  the method of gradient descent lies in the fact
            that some analytic \footnote{We use the term analytic quite
            recklessly.} function $\vf{F}(\alpha)$ decreases fastest in the direction
            of $-\nabla_\alpha \cdot \vf{F}(\alpha)$. This means that we could eventually
            get to an ensuing minimum by trailing along the pathway of the
            following recursive relation,
            \begin{equation}
                \vfg{\alpha}_{k+1}
                = \vfg{\alpha}_k - \gamma \nabla_{\alpha}\cdot \vf{F}(\alpha_k).
                \label{eq:parameter_difference}
            \end{equation}
            If $\gamma$ is small enough we should have $\vf{F}(\alpha_k) \geq
            \vf{F}(\alpha_{k+1})$ for all $k$ and hopefully we will experience
            convergence to the desired minimum.

            For our special case we only have one variational parameter,
            $\alpha$.  The expression for the derivative is,
            \begin{align}
                \nabla_{\alpha}\expval{E_L}
                =
                2\left(
                    \expval{E_L\frac{1}{\Psi_T}\nabla_{\alpha}\Psi_T}
                    - \expval{\frac{1}{\Psi_T} \nabla_{\alpha}\Psi_T}
                    \expval{E_L}
                \right).
                \label{eq:variational_parameter_gradient}
            \end{align}

            To evaluate this expression we sample the local energy, the
            parameter gradient of the wavefunction, and the combined expression
             of these at each Monte Carlo cycle.

            \subsubsection{Stability of the algorithm}
                When employing the method of gradient descent to locate a
                minimum value for the variational parameters we expect some
                issues due to stability of the Monte Carlo methods and the
                steepness of the expected energy curves. If our guess of the
                location of the minimum is far off we can expect the method to
                diverge as the gradient will become huge. This will yield a
                new value for $\vfg{\alpha}$ in
                \autoref{eq:parameter_difference} which will be further away and
                might result in a local minimum ``trapping'' the method. To
                prevent this from happening we have added a limit to the allowed
                change in $\vfg{\alpha}$. If the change in $\vfg{\alpha}$ is
                larger than a set order of magnitude we keep dividing
                $\vfg{\alpha}$ with this number until this criteria is
                fulfilled.

\section{Results}

    \subsection{Non-Interacting Systems}
        Initially we are interested in seeing how our implementation performs
        and to what extent it functions properly.  the logical thing to do would
        be to look at a non-interacting system with a simple Gaussian
        wavefunction and harmonic oscillator potential. Such a system has a
        simple closed-form analytic solution\footnote{A fact that every
        undergraduate physics student should be well aware of after finishing
        their first course in quantum physics}, which is further simplified by
        the "god-given" analytic units
        (\autoref{eq:closed_form_natural_units_local_energy}). This expression
        is minimised for $\alpha = \frac{1}{2}\omega$. We also set $\omega = 1$
        and get an expression for the expected minimum local energy,
        \begin{equation}
            E_L(\vb{r}) = \frac{dN}{2},
        \end{equation}
        which will be used as the benchmark we are ultimately aiming for.

        We will start without importance sampling, that is by employing the
        simpler and more naïve ``brute force'' Monte Carlo sampling. Moreover, we
        will compute the value for the local energy analytically as well as with
        a numerical approach.  In the numerical approach we employ the well-know
        approximation for the second derivative,
        \begin{equation}
                f''(x) \approx \frac{f(x-h) - 2f(x) + f(x+h)}{h^2}.
        \end{equation}

        \begin{table}
            \centering
            \caption{One particle in one dimension for the analytic expression
            with $2^{21}$ Monte Carlo cycles and step length of $0.5$.}
            \begin{ruledtabular}
                \begin{tabular}{cccccc}
                    $\alpha$ & $\langle  E_L\rangle$ & $\sigma$ & $\sigma_b$
                    & $A$ & $t_{\text{CPU}} [\si{\second}]$ \\
                    \hline
                    0.30&0.56244&0.00026&0.00129&0.892&0.43\\
                    0.34&0.53652&0.00019&0.00094&0.885&0.38\\
                    0.38&0.51863&0.00013&0.00063&0.878&0.37\\
                    0.42&0.50805&0.00009&0.00038&0.872&0.38\\
                    0.46&0.50201&0.00004&0.00017&0.866&0.37\\
                    0.50&0.50000&0.00000&0.00000&0.860&0.37\\
                    0.54&0.50183&0.00004&0.00015&0.855&0.38\\
                    0.58&0.50566&0.00007&0.00029&0.850&0.38\\
                    0.62&0.51175&0.00011&0.00041&0.845&0.37\\
                    0.66&0.51906&0.00014&0.00052&0.840&0.37\\
                    0.70&0.52883&0.00017&0.00062&0.836&0.37\\
                \end{tabular}
            \end{ruledtabular}
            \label{tab:1D1N_analytic}
        \end{table}

        \begin{table}
            \centering
            \caption{One particle in one dimension for the numeric expression
            with $2^{21}$ Monte Carlo cycles and a step length of $0.5$.}
            \begin{ruledtabular}
                \begin{tabular}{cccccc}
                    $\alpha$ & $\langle  E_L\rangle$ & $\sigma$ & $\sigma_b$
                    & $A$&$t_{\text{CPU}} [\si{\second}]$ \\
                    \hline
                    0.30&0.56675&0.00026&0.00135&0.891&0.53\\
                    0.34&0.53714&0.00019&0.00096&0.885&0.52\\
                    0.38&0.51836&0.00013&0.00062&0.878&0.52\\
                    0.42&0.50717&0.00009&0.00038&0.872&0.53\\
                    0.46&0.50184&0.00004&0.00017&0.866&0.52\\
                    0.50&0.50000&0.00001&0.00001&0.860&0.52\\
                    0.54&0.50133&0.00004&0.00015&0.855&0.52\\
                    0.58&0.50520&0.00007&0.00028&0.850&0.52\\
                    0.62&0.51120&0.00011&0.00039&0.845&0.52\\
                    0.66&0.51896&0.00014&0.00052&0.840&0.52\\
                    0.70&0.52794&0.00017&0.00061&0.835&0.51\\
                \end{tabular}
            \end{ruledtabular}
            \label{tab:1D1N_numeric}
        \end{table}

        \autoref{tab:1D1N_analytic} shows the expected local energy
        $\expval{E_L}$, standard deviations $\sigma$, $\sigma_b$, the share of
        accepted Monte Carlo reorientations $A$ and cpu time for the analytic
        computation at different values for the variational parameter $\alpha$.
        \autoref{tab:1D1N_numeric} shows the same numbers but for the numeric
        scheme. The first column of standard deviations $\sigma$ are the "naïve"
        standard deviations, while $\sigma_b$ marks the standard deviations
        computed with the blocking method. One can clearly see, in both the
        analytic and numeric case, that the former standard deviations are lower
        than the latter, meaning that autocorrelated data can lead to a lower
        perceived uncertainty than what is reality. The blocking method takes
        this fact into account and for this reason, all other standard
        deviations will be computed with the blocking method from here on.

        The first initial observation of note is that we have found an optimum
        for the system in terms of a minimum energy at $\alpha=\frac{1}{2}$.
        Moreover, notice how the standard deviation disappears completely.  This
        should already be apparent from
        \autoref{eq:closed_form_natural_units_local_energy}, as the term
        involving particle positions disappears for the optimal $\alpha$.

        \begin{table}
            \caption{Comparison of energy, standard deviation and the CPU time
            for $\alpha = 0.5$ (the correct minimum) for the analytic and
            numeric schemes in three dimensions and $N$ particles. The step
            length is $0.5$.}
            \centering
            \begin{ruledtabular}
                \begin{tabular}{r|rrr|rrr}
                    & \multicolumn{3}{c|}{Analytic}
                    & \multicolumn{3}{c}{Numeric} \\
                    \hline
                    $N$
                    & $\langle E_L\rangle$ & $\sigma_b$
                    & $t_{\text{CPU}} [\si{\second}]$
                    & $\langle E_L\rangle$ & $\sigma_b$
                    & $t_{\text{CPU}} [\si{\second}]$ \\
                    \hline
                    1 & 1.5 & 0.0 & 0.50
                    & 1.49997 & 0.00008 & 0.73 \\
                    10 & 15.0 & 0.0 & 0.73
                    & 14.98278 & 0.00867 & 6.40 \\
                    100 & 150.0 & 0.0 & 4.13
                    & 149.81453 & 0.81501 & 387.60 \\
                    500 & 750.0 & 0.0 & 19.16
                    & 773.85435 & 20.69789 & 9595.02
                \end{tabular}
            \end{ruledtabular}
            \label{tab:initial_optimum_comparison}
        \end{table}


        Systems of optimum variational parameter warrants further investigation,
        as summed up in \autoref{tab:initial_optimum_comparison}. The analytical
        results are quite predictable and monotonous; the expected local energy
        $\expval{E_L}$ is perfectly proportional by a factor $\frac{3}{2}$ to
        the number of particles and the standard deviation $\sigma_b$ is zero
        for systems of any number of particles. The numerical results tells a
        more tortuous tale. As the number of particles increase, the uncertainty
        also increases rapidly. For the largest system, with $N = 500$
        particles, the energy misses its target critically - the true energy is
        more than one standard deviation away. Lastly, the cpu time is notably
        higher for the numeric scheme.

        In the appendices,  \autoref{fig:initial_problem_b} shows how both the
        analytic and numeric approach matches the exact expression for the
        energy as a function of $\alpha$ for systems of increasing size.  We see
        also here that the uncertainty increases in the numeric scheme as the
        system gets larger.

        \subsubsection{Introducing Importance Sampling}

            Next we will introduce importance sampling, the first prediction for
            importance sampling compared to brute, naïve method is that we
            should approach an equilibrium much faster because we are now making
            moves in a smarter way. This prediction is confirmed in
            \autoref{fig:problem_c1} and \autoref{fig:problem_c2}.

            \begin{figure}
                \centering
                    \includegraphics[width=244px]
                    {../data/figures/problem_c1.pdf}
                    \caption{As the number of Monte Carlo cycles increases the
                    standard deviation of the local energies decrease. This
                    effect is stronger when the importance sampling scheme is
                    employed.}
                    \label{fig:problem_c1}
            \end{figure}

            \begin{figure}
                \centering
                    \includegraphics[width=244px]
                    {../data/figures/problem_c2.pdf}
                    \caption{The absolute difference between the expected energy
                    and the exact energy for both brute force and importance
                    sampling. }
                    \label{fig:problem_c2}
            \end{figure}

            \autoref{fig:problem_c1} shows the standard deviation of the local
            energies for increasing number of Monte Carlo cycles for both the
            regular sampling and the importance sampling methods. We see that
            the standard deviation decreases for both methods for a larger
            number of Monte Carlo cycles, but the standard deviation is always
            lower for the importance sampling method. This is in accordance with
            our expectations.

            We have also compared the expected energy with the exact energy for
            different number of Monte Carlo cycles. The result of this analysis
            is shown in \autoref{fig:problem_c2}. We see that both methods will
            evnetually result in expected energies that are equal to the exact
            energies as $\abs{E - \expval{E}} \to 0$. However, for the method of
            importance sampling the error is consistently lower.

            \begin{table}
                \caption{Importance sampled simulations of $n=10$ particles in
                $d=3$ dimensions. Average acceptance ratios and standard
                deviations for eleven alpha values between $0.3$ and $0.7$.}
                \centering
                    \begin{ruledtabular}
                        \begin{tabular}{cccc}
                            $\delta t$ & $\bar{\sigma}_b$ & $\bar{A}$
                            & $\expval{E}\left(\alpha = 1/2\right)$ \\
                            \hline
                            $2^{+3}$ & 0.09363 & 0.005 & 15.000 \\
                            $2^{+2}$ & 0.01549  & 0.041 & 15.000 \\
                            $2^{+1}$ & 0.00706 & 0.216 & 15.000 \\
                            $2^{\ 0}$ & 0.00675 & 0.510 & 15.000 \\
                            $2^{-1}$ & 0.00932 & 0.738 & 15.000 \\
                            $2^{-2}$ & 0.01578 & 0.866 & 15.000 \\
                            $2^{-3}$ & 0.02723 & 0.933 & 15.000 \\
                            $2^{-4}$ & 0.05016 & 0.966 & 15.000 \\
                            $2^{-5}$ & 0.08474 & 0.983 & 15.000 \\
                            $2^{-6}$ & 0.11555 & 0.992 & 15.000 \\
                            $2^{-7}$ & 0.17070 & 0.996 & 15.000 \\
                        \end{tabular}
                    \end{ruledtabular}
                    \label{tab:imp_time_step}
            \end{table}

            \autoref{tab:imp_time_step} shows the standard deviations and
            acceptance ratios for different time steps. The simulations in this
            tabel are from the same span $\alpha \in [0.3, 07]$ as in
            \autoref{tab:1D1N_analytic} and \autoref{tab:1D1N_numeric}. The
            optimum energies for $\alpha = \frac{1}{2}$ gives the same result
            for importance sampling as for the naïve way of sampling. Notice
            that by lowering the time step, the uncertainty starts to decrease
            and then starts to increase again. Moreover, the acceptance ratios
            increases as the time step decreases.

    \subsection{Interacting elliptical harmonic oscillator}
        We now make a change to the simple systems in the previous section in
        order to make everything a bit more interesting. The potential in the
        one-body Hamiltonian is perturbed in the $z$-direction according to
        \autoref{eq:scaled_hamiltonian} by setting $\lambda = \sqrt{8} \approx
        2.82843$. This corresponds to enclosing particles of the simulation in
        an elliptical trap instead of a spherical trap.

        We also choose a new trial wavefunction with the elliptical gaussian
        single particle functions in \autoref{eq:scaled_spf} and the Jastrow
        factor. The hard sphere radius is set to $a/a_{\text{ho}} = 0.0043$.

        Due to similarities with the spherical harmonic oscillator
        system we make a guess that the true minimum of this new system should
        be situated close to the minimum of the previous system. Choosing seven
        values for $\alpha \in [0.2, 0.7]$ we run importance sampling on this
        system for $N = \{10, 50, 100\}$ particles in $d = 3$ dimensions using
        $2^{21}$ Monte Carlo cycles\footnote{This turned out to be a little bit
        overkill due to the fast convergence of importance sampling.} and an
        additional $10\%$ of the Monte Carlo cycles for thermalization of the
        system prior to doing any sampling. We have used $\delta t = 0.1$ yields
        a high acceptance ratio, but slower convergence.

        In \autoref{tab:10_interacting} the system is set to have $N = 10$
        interacting particles. Notice first that the energy is higher in
        the interacting system than for a comparable non-interacting system
        (\autoref{tab:initial_optimum_comparison}).  Moreover, the standard
        deviation gets lower as the system is close to an optimal variational
        parameter.

        \begin{table}
            \caption{Simulation results for $N = 10$ bosonic interacting,
            elliptical, harmonic oscillators.}
            \centering
            \begin{ruledtabular}
                \begin{tabular}{ccccc}
                    $\alpha$ & $\langle  E_L\rangle$ & $\sigma_b$
                    &$A$ & $t_{\text{CPU}} [\si{\second}]$ \\
                    \hline
                    0.2&35.17548&0.05399&0.990%98995
                    &24.5\\%48868\\
                    0.3&27.62004&0.02311&0.981%98187
                    &13.6\\%65132\\
                    0.4&24.97850&0.00827&0.972%97248
                    &13.5\\%54666\\
                    0.5&24.39877&0.00030&0.961%96140
                    &13.4\\%38388\\
                    0.6&24.83863&0.00604&0.950%94998
                    &13.3\\%31570\\
                    0.7&25.82855&0.01059&0.938%93766
                    &13.2\\%20579\\
                    0.8&27.22895&0.01436&0.924%92375
                    &13.1\\%11933\\
                \end{tabular}
            \end{ruledtabular}
            \label{tab:10_interacting}
        \end{table}

        The result of simulations with a higher number of particles, $N=50$ and
        $N=100$, are  shown in \autoref{tab:50_interacting} and
        \autoref{tab:100_interacting} respectively. We observe that the energy
        for systems of interacting particles increases rapidly as the size of
        the system increases.

        \begin{table}
            \caption{Simulation results for $N = 50$ bosonic interacting,
            elliptical, harmonic oscillators.}
            \centering
            \begin{ruledtabular}
                \begin{tabular}{ccccc}
                    $\alpha$ & $\langle  E_L\rangle$ & $\sigma_b$
                    & $A$ & $t_{\text{CPU}} [\si{\second}]$ \\
                    \hline
                    0.2&181.69371&0.25533&0.987%98756
                    &199.9\\%95366\\
                    0.3&142.61591&0.10782&0.978%97849
                    &195.8\\%80928\\
                    0.4&129.88615&0.04051&0.969%96862
                    &190.4\\%44553\\
                    0.5&127.29926&0.00595&0.957%95749
                    &195.1\\%10368\\
                    0.6&129.97630&0.03300&0.946%94595
                    &193.2\\%22755\\
                    0.7&135.67382&0.05785&0.933%93330
                    &192.0\\%191.96391\\
                    0.8&143.23238&0.07286&0.921%92061
                    &192.7\\%72399\\
                \end{tabular}
            \end{ruledtabular}
            \label{tab:50_interacting}
        \end{table}

        \begin{table}
            \caption{Simulation results for $N = 100$ bosonic interacting,
            elliptical, harmonic oscillators.}
            \centering
            \begin{ruledtabular}
                \begin{tabular}{ccccc}
                    $\alpha$ & $\langle  E_L\rangle$ & $\sigma_b$
                    &$A$ & $t_{\text{CPU}} [\si{\second}]$ \\
                    \hline
                    0.2&375.04401&0.51475&0.985%98527
                    &745.2\\%21438\\
                    0.3&296.76879&0.20214&0.975%97534
                    &741.0\\%04425\\
                    0.4&270.60830&0.08113&0.945%94522
                    &723.1\\%06823\\
                    0.5&266.37263&0.02020&0.953%95314
                    &702.8\\%71000\\
                    0.6&272.51171&0.07366&0.941%94112
                    &689.1\\%12862\\
                    0.7&285.10285&0.11381&0.929%92895
                    &707.5\\%48248\\
                    0.8&301.40609&0.15458&0.916%91648
                    &707.9\\%86118\\
                \end{tabular}
            \end{ruledtabular}
            \label{tab:100_interacting}
        \end{table}

        \subsubsection{Finding the minimum}
            Using the method of gradient descent we choose a set of six starting
            $\alpha_0 \in [0.2, 0.8]$ and try to locate a value for $\alpha$
            where the gradient goes to zero. We looked at $N = 10$ with $d = 3$,
            $\omega = 1$ and $\beta = \lambda = \sqrt{8}$. This yields the value
            for the minimum $\alpha$ to be
            \begin{align}
                \alpha = 0.49744 \pm 0.00002.
                \label{eq:optimal_interacting_alpha}
            \end{align}
            In \autoref{fig:gradient_descent_interacting} we can see how the
            conjugate gradient method ``moves'' to find the optimal value of
            $\alpha$.

            \begin{figure}
                \includegraphics[width=244px]{../data/figures/problem_f.pdf}
                \caption{In this figure we can see the convergence of different
                starting values for $\alpha$ towards the true value of $\alpha$
                that minimizes the expected energy in the interacting case.}
                \label{fig:gradient_descent_interacting}
            \end{figure}

        \subsubsection{One-Body Densities}
            We now look at one-body densities for the value of $\alpha$ found in
            \autoref{eq:optimal_interacting_alpha}.

            \begin{table}
                \caption{Comparison of the elliptical system with an elliptical
                gaussian trial wavefunction with and without the Jastrow
                factor.}
                \centering
                \begin{ruledtabular}
                    \begin{tabular}{r|rr|rr}
                        & \multicolumn{2}{c|}{Interacting}
                        & \multicolumn{2}{c}{Non-interacting} \\
                        \hline
                        $N$ & $\expval{E}$ & $\sigma_b$ & $\expval{E}$
                        & $\sigma_b$ \\
                        \hline
                        10 & 24.3986 & 0.0003 & 24.1418 & 0.0002 \\
                        50 & 127.2852 & 0.0077 & 120.7129 & 0.0012 \\
                        100 & 266.2904 & 0.0297 & 241.4248 & 0.0025
                    \end{tabular}
                \end{ruledtabular}
                \label{tab:10_optimal}
            \end{table}

            \begin{figure}
                \includegraphics[width=244px]
                {../data/figures/problem_g_100N.pdf}
                \caption{In this figure the interacting and non-interacting
                densities correspond to the elliptical potential with and
                without the Jastrow factor respectively. The ``ideal'' system is
                the spherical potential with the non-interacting Gaussian trial
                wavefunction.}
                \label{fig:one_body_density_100}
            \end{figure}


\section{Discussion}

    \subsection{Validity of Results}
        The non-interacting tests compared against the analytic "solution"
        showed to a very high degree the validity of the method. One can see
        this quite clearly from \autoref{fig:initial_problem_b}. Because we get
        results from simulations that correspond to the analytic results we can
        be fairly certain that the method can be expanded upon to include
        attributes outside the realm problems that are analytically solvable.
        The systems with elliptic traps and interacting particles are such
        systems.

    \subsection{Energy per particle}
        For the non-interacting harmonic oscillator system there is no
        difference between a new ``particle'' and new dimensions
        \footnote{This is not entirely true in the case of our implementation of
        the Metropolis algorithms as we draw a single random particle and move
        all dimensions instead of iterating over all particles and all
        dimensions.}. This means that the inclusion of a new particle or
        dimension yields a linear scaling in the energy. Thus if we compute the
        energy per particle for systems of varying size we expect to get the
        same energy. This is most easily seen in \autoref{fig:initial_problem_b}
        and in the expression for the exact energy \autoref{eq:exact_energy}.
        This, however, is \emph{not} the case in the interacting system due to
        the Jastrow factor. Looking at \autoref{tab:10_interacting} and
        \autoref{tab:100_interacting} we see that multiplying the expected
        energy of theformer with $10$ yields a lower number than the latter. We
        can also see this behavior in \autoref{fig:interacting}, where the
        energy per particle for $N = \{10, 50, 100\}$ have been plotted in the
        same figure.

        \begin{figure}
            \includegraphics[width=244px]{../data/figures/problem_e.pdf}
            \caption{Local energies as a function of variational parameter
            $\alpha$ for different number of particles.}
            \label{fig:interacting}
        \end{figure}

\section{Summary Remarks}


\appendix
\section{Brute Force Metropolis-Hastings}

    \begin{figure*}
        \makebox[\textwidth][c]{
            \includegraphics[width=1.2\textwidth]{../data/figures/problem_b.pdf}
        }
        \caption{Comparison of analytic and numeric results with the exact
        expression for the energy as a function of $\alpha$. The plots show the
        standard deviation from the blocking method as error ticks. In the
        figure $N$ is the number of particles and $d$ the number of dimensions.}
        \label{fig:initial_problem_b}
    \end{figure*}

\section{Variational parameter gradient of the expectation energy}
    Here we show how to arrive at the expression shown in
    \autoref{eq:variational_parameter_gradient}. This requires us to restrict
    our view to real trial wavefunctions, i.e., $\Psi_T^{*} = \Psi_T$, which is
    the case for the wavefunctions we are exploring.
    \begin{align}
        \nabla_{\alpha}\expval{E_L}
        &=
        \nabla_{\alpha}
        \para{
            \frac{
                \int\dd\vf{r}\Psi_T^{*}H\Psi_T
            }{
                \int\dd\vf{r}|\Psi_T|^2
            }
        }
        \\
        &=
        -
        \frac{
            2
            \para{
                \int\dd\vf{r}\Psi_T H\Psi_T
            }
        }{\para{\int\dd\vf{r}\Psi_T}^2}
        \int\dd\vf{r}\Psi_T\nabla_{\alpha}\Psi_T
        \\
        &\qquad
        +
        \frac{2}{\int\dd\vf{r}\Psi_T^2}
        \int\dd\vf{r}\Psi_TH\brak{
            \nabla_{\alpha}\Psi_T
        }
        \\
        &=
        - 2\expval{E_L}
        \frac{1}{
            \int\dd\vf{r}\Psi_T^2
        }
        \int\dd\vf{r}\Psi_T^2\para{
            \frac{\nabla_{\alpha}\Psi_T}{\Psi_T}
        }
        \\
        &\qquad
        +
        \frac{2}{\int\dd\vf{r}\Psi_T^2}
        \int\dd\vf{r}\Psi_T^2 E_L\para{
            \frac{\nabla_{\alpha}\Psi_T}{\Psi_T}
        }.
    \end{align}
    We now use the definition of the expectation value to group terms together.
    We are thus left with
    \begin{align}
        \nabla_{\alpha}\expval{E_L}
        =
        2
        \para{
            \expval{
                E_L\frac{1}{\Psi_T}\nabla_{\alpha}\Psi_T
            }
            - \expval{E_L}
            \expval{\frac{1}{\Psi_T}\nabla_{\alpha}\Psi_T}
        },
    \end{align}
    which is what we wanted to show.

\bibliography{references}



\end{document}
